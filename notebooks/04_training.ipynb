{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e71fcca",
   "metadata": {},
   "source": [
    "### **Tecnológico de Monterrey**\n",
    "\n",
    "#### **Maestría en Inteligencia Artificial Aplicada**\n",
    "#### **Clase**: Operaciones de Aprendizaje Automático\n",
    "#### **Docentes**: Dr. Gerardo Rodríguez Hernández | Mtro. Ricardo Valdez Hernández | Mtro. Carlos Alberto Vences Sánchez\n",
    "\n",
    "##### **Actividad**: Proyecto: Avance (Fase 1) **Notebook**: Modelo de aprendizaje automático\n",
    "##### **Equipo 25**:\n",
    "| Nombre | Matrícula |\n",
    "|--------|-----------|\n",
    "| Rafael Becerra García | A01796211 |\n",
    "| Andrea Xcaret Gómez Alfaro | A01796384 |\n",
    "| David Hernández Castellanos | A01795964 |\n",
    "| Juan Pablo López Sánchez | A01313663 |\n",
    "| Osiris Xcaret Saavedra Solís | A01795992 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889e3ed",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    "\n",
    "**Analisis de Requerimientos**\n",
    "**Tarea**: Analiza la problemática a resolver siguiendo la liga con la descripción del dataset asignado.\n",
    "\n",
    "**Manipulación y preparación de datos**\n",
    "**Tarea**: Realizar tareas de Exploratory Data Analysis (EDA)  y limpieza de datos utilizando herramientas y bibliotecas específicas (Python, Pandas, DVC, Scikitlearn, etc.)\n",
    "\n",
    "**Exploración y preprocesamiento de datos**\n",
    "**Tarea**: Explorar y preprocesar los datos para identificar patrones, tendencias y relaciones significativas.\n",
    "\n",
    "**Versionado de datos**\n",
    "**Tarea**: Aplicar técnicas de versionado de datos para asegurar reproducibilidad y trazabilidad.\n",
    "\n",
    "**Construcción, ajuste y evaluación de Modelos de Machine Learning**\n",
    "**Tarea**: Construir, ajustar y evaluar modelos de Machine Learning utilizando técnicas y algoritmos apropiados al problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45c9162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 17:49:06 INFO mlflow.tracking.fluent: Experiment with name 'Obesity_Classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente:\n",
      "Train: (1472, 23), Test: (632, 23)\n"
     ]
    }
   ],
   "source": [
    "# --- Inicialización --- #\n",
    "\n",
    "# Librerías\n",
# -*- coding: utf-8 -*-
"""
04_training.py — Model selection con búsqueda AUTO (Grid/Random)
- Decide GridSearchCV o RandomizedSearchCV automáticamente según el tamaño del grid
  (combinaciones > max_combinations => randomized con n_iter)
- Permite forzar modo global (search.mode) o por modelo (search.per_model.<name>)
- Compara: LogisticRegression, RandomForest, HistGradientBoosting, SVC-RBF
- Métrica principal: f1_macro (desbalance)
- Artefactos: metrics.json, classification_report.csv, confusion_matrix.png,
  final_model_comparison.csv (append), models/best_model.joblib
"""

import os
import json
import math
import warnings
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
from ruamel.yaml import YAML

from sklearn.metrics import (
    f1_score, accuracy_score, precision_score, recall_score,
    classification_report, confusion_matrix
)
from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.svm import SVC
import joblib
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")


# ---------- Utilidades ----------

def load_params(params_path: str = "params.yaml") -> Dict[str, Any]:
    yaml = YAML(typ="safe")
    with open(params_path, "r", encoding="utf-8") as f:
        return yaml.load(f)

def ensure_dirs():
    Path("reports/figures").mkdir(parents=True, exist_ok=True)
    Path("models").mkdir(parents=True, exist_ok=True)

def read_data(cfg: dict):
    X_train = pd.read_csv(cfg["data"]["X_train_path"])
    X_test = pd.read_csv(cfg["data"]["X_test_path"])
    y_train = pd.read_csv(cfg["data"]["y_train_path"]).squeeze().astype(str)
    y_test = pd.read_csv(cfg["data"]["y_test_path"]).squeeze().astype(str)
    return X_train, X_test, y_train, y_test

def get_cv(cfg: dict):
    return StratifiedKFold(
        n_splits=cfg["cv"]["n_splits"],
        shuffle=cfg["cv"]["shuffle"],
        random_state=cfg["cv"]["random_state"]
    )

def plot_confusion_matrix(y_true, y_pred, labels, out_path: str):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots(figsize=(9, 7))
    im = ax.imshow(cm, interpolation="nearest", cmap="Blues")
    ax.figure.colorbar(im, ax=ax)
    ax.set(
        xticks=np.arange(cm.shape[1]),
        yticks=np.arange(cm.shape[0]),
        xticklabels=labels, yticklabels=labels,
        ylabel="True label", xlabel="Predicted label",
        title="Confusion Matrix (Test)"
    )
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], "d"),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    fig.savefig(out_path, bbox_inches="tight", dpi=180)
    plt.close(fig)

def dump_json(obj, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)

def count_param_combinations(grid: Dict[str, list]) -> int:
    """Cuenta combinaciones de un grid (producto de longitudes). Soporta valores None/escalares."""
    total = 1
    for _, values in grid.items():
        # si el valor no es lista, trátalo como 1 opción
        if isinstance(values, (list, tuple)):
            total *= max(len(values), 1)
        else:
            total *= 1
    return int(total)


# ---------- Definición de modelos + grids ----------

def build_models_and_grids(cfg: dict):
    use = cfg["models"]
    grids_cfg = cfg["grids"]

    models = {}
    param_grids = {}

    if use.get("logistic_regression", True):
        models["logistic_regression"] = LogisticRegression(multi_class="auto", n_jobs=cfg["search"]["n_jobs"])
        param_grids["logistic_regression"] = {
            "C": grids_cfg["logistic_regression"]["C"],
            "penalty": grids_cfg["logistic_regression"]["penalty"],
            "solver": grids_cfg["logistic_regression"]["solver"],
            "max_iter": grids_cfg["logistic_regression"]["max_iter"],
            "class_weight": grids_cfg["logistic_regression"]["class_weight"],
        }

    if use.get("random_forest", True):
        models["random_forest"] = RandomForestClassifier(
            random_state=cfg["cv"]["random_state"], n_jobs=cfg["search"]["n_jobs"]
        )
        param_grids["random_forest"] = {
            "n_estimators": grids_cfg["random_forest"]["n_estimators"],
            "max_depth": grids_cfg["random_forest"]["max_depth"],
            "min_samples_split": grids_cfg["random_forest"]["min_samples_split"],
            "min_samples_leaf": grids_cfg["random_forest"]["min_samples_leaf"],
            "class_weight": grids_cfg["random_forest"]["class_weight"],
        }

    if use.get("hist_gradient_boosting", True):
        models["hist_gradient_boosting"] = HistGradientBoostingClassifier(
            random_state=cfg["cv"]["random_state"]
        )
        param_grids["hist_gradient_boosting"] = {
            "learning_rate": grids_cfg["hist_gradient_boosting"]["learning_rate"],
            "max_depth": grids_cfg["hist_gradient_boosting"]["max_depth"],
            "max_leaf_nodes": grids_cfg["hist_gradient_boosting"]["max_leaf_nodes"],
            "min_samples_leaf": grids_cfg["hist_gradient_boosting"]["min_samples_leaf"],
            "l2_regularization": grids_cfg["hist_gradient_boosting"]["l2_regularization"],
        }

    if use.get("svc_rbf", True):
        models["svc_rbf"] = SVC()
        param_grids["svc_rbf"] = {
            "C": grids_cfg["svc_rbf"]["C"],
            "gamma": grids_cfg["svc_rbf"]["gamma"],
            "probability": grids_cfg["svc_rbf"]["probability"]
        }

    return models, param_grids


# ---------- Selección de búsqueda (AUTO / GRID / RANDOM) ----------

def choose_mode_for_model(global_mode: str, per_model: dict, model_name: str) -> str:
    """Devuelve el modo efectivo para el modelo (per_model override o global)."""
    m = (per_model or {}).get(model_name)
    if m in {"auto", "grid", "random"}:
        return m
    if global_mode in {"auto", "grid", "random"}:
        return global_mode
    return "auto"

def make_search(estimator, grid, cfg, cv, model_name: str):
    scoring = cfg["search"]["scoring"]
    n_jobs = cfg["search"]["n_jobs"]
    verbose = cfg["search"]["verbose"]
    n_iter = cfg["search"]["n_iter"]
    max_combos = cfg["search"]["max_combinations"]
    global_mode = cfg["search"]["mode"]
    per_model = cfg["search"].get("per_model", {})

    mode = choose_mode_for_model(global_mode, per_model, model_name)
    n_combos = count_param_combinations(grid)

    if mode == "auto":
        chosen = "grid" if n_combos <= max_combos else "random"
    else:
        chosen = mode

    print(f" -> [{model_name}] grid combinations: {n_combos} | mode: {chosen}")

    if chosen == "random":
        # limitar n_iter al número de combinaciones (si es pequeño)
        n_iter_eff = min(n_iter, max(n_combos, 1))
        return RandomizedSearchCV(
            estimator=estimator,
            param_distributions=grid,
            n_iter=n_iter_eff,
            scoring=scoring,
            cv=cv,
            n_jobs=n_jobs,
            verbose=verbose,
            refit=True,
            random_state=cfg["cv"]["random_state"]
        )
    else:  # grid
        return GridSearchCV(
            estimator=estimator,
            param_grid=grid,
            scoring=scoring,
            cv=cv,
            n_jobs=n_jobs,
            verbose=verbose,
            refit=True
        )


# ---------- Entrenamiento ----------

def main():
    ensure_dirs()
    cfg = load_params("params.yaml")

    # 1) Datos
    X_train, X_test, y_train, y_test = read_data(cfg)
    labels_order = sorted(y_train.unique().tolist())

    # 2) CV
    cv = get_cv(cfg)

    # 3) Modelos y grids
    models, param_grids = build_models_and_grids(cfg)

    results = []
    best_global = {"score": -np.inf, "name": None, "estimator": None}

    # 4) Loop por modelo → búsqueda auto/forzada
    for name, est in models.items():
        print(f"\n=== Searching {name} ===")
        grid = param_grids[name]
        search = make_search(est, grid, cfg, cv, name)
        search.fit(X_train, y_train)

        print(f"[{name}] best params: {search.best_params_}")
        print(f"[{name}] best cv {cfg['search']['scoring']}: {search.best_score_:.4f}")

        # Test
        y_pred = search.predict(X_test)
        f1 = f1_score(y_test, y_pred, average="macro")
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, average="macro", zero_division=0)
        rec = recall_score(y_test, y_pred, average="macro", zero_division=0)

        results.append({
            "model": name,
            "mode_used": choose_mode_for_model(cfg["search"]["mode"], cfg["search"].get("per_model", {}), name),
            "grid_combinations": count_param_combinations(grid),
            "best_params": search.best_params_,
            "cv_f1_macro": float(search.best_score_),
            "test_f1_macro": float(f1),
            "test_accuracy": float(acc),
            "test_precision_macro": float(prec),
            "test_recall_macro": float(rec)
        })

        if f1 > best_global["score"]:
            best_global.update({"score": f1, "name": name, "estimator": search.best_estimator_})

    # 5) Artefactos del mejor modelo
    best_name = best_global["name"]
    best_est = best_global["estimator"]
    print(f"\n*** Best model (test f1_macro): {best_name} = {best_global['score']:.4f}")

    y_pred_best = best_est.predict(X_test)
    cls_rep = classification_report(y_test, y_pred_best, output_dict=True, zero_division=0)

    # Guardar
    joblib.dump(best_est, "models/best_model.joblib")

    metrics = {
        "best_model": best_name,
        "test_f1_macro": float(f1_score(y_test, y_pred_best, average="macro")),
        "test_accuracy": float(accuracy_score(y_test, y_pred_best)),
        "test_precision_macro": float(precision_score(y_test, y_pred_best, average="macro", zero_division=0)),
        "test_recall_macro": float(recall_score(y_test, y_pred_best, average="macro", zero_division=0))
    }
    dump_json(metrics, "reports/metrics.json")

    pd.DataFrame(cls_rep).to_csv("reports/classification_report.csv", index=True)

    plot_confusion_matrix(y_test, y_pred_best, labels_order, "reports/figures/confusion_matrix.png")

    cmp_path = Path("reports/final_model_comparison.csv")
    df_results = pd.DataFrame(results)
    if cmp_path.exists():
        prev = pd.read_csv(cmp_path)
        out = pd.concat([prev, df_results], ignore_index=True)
    else:
        out = df_results
    out.to_csv(cmp_path, index=False)

    print("\nArtefactos guardados en reports/ y models/")
    print("- reports/metrics.json")
    print("- reports/classification_report.csv")
    print("- reports/figures/confusion_matrix.png")
    print("- reports/final_model_comparison.csv")
    print("- models/best_model.joblib")


if __name__ == "__main__":
    main()

    
